{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"GBQ - 0.1.5 Example from gbq import BigQuery # BigQuery project id as listed in the Google Developers Console. project_id = 'project_id' # BigQuery dataset id as listed in the Google Developers Console. dataset_id = 'dataset_id' # BigQuery table/view id as listed in the Google Developers Console. structure_id = 'structure_id' # BigQuery structure definition as defined in the Google Developers Console. json_schema = { \"type\" : \"table\" , \"schema\" : [{ \"id\" : \"integer\" }]} # Service account email address as listed in the Google Developers Console. svc_account = '{\"type\": \"service_account\", \"project_id\": \"project_id\"}' bq = BigQuery ( svc_account = svc_account , project = project_id ) bq . create_or_update_structure ( project_id , dataset_id , structure_id , json_schema ) Where to Start? To learn the basics of how to start using gbq , read the Getting Started page. Detailed Documentation To learn more about the various ways gbq can be used, read the Usage Guide page.","title":"Overview"},{"location":"#gbq-015","text":"","title":"GBQ - 0.1.5"},{"location":"#example","text":"from gbq import BigQuery # BigQuery project id as listed in the Google Developers Console. project_id = 'project_id' # BigQuery dataset id as listed in the Google Developers Console. dataset_id = 'dataset_id' # BigQuery table/view id as listed in the Google Developers Console. structure_id = 'structure_id' # BigQuery structure definition as defined in the Google Developers Console. json_schema = { \"type\" : \"table\" , \"schema\" : [{ \"id\" : \"integer\" }]} # Service account email address as listed in the Google Developers Console. svc_account = '{\"type\": \"service_account\", \"project_id\": \"project_id\"}' bq = BigQuery ( svc_account = svc_account , project = project_id ) bq . create_or_update_structure ( project_id , dataset_id , structure_id , json_schema )","title":"Example"},{"location":"#where-to-start","text":"To learn the basics of how to start using gbq , read the Getting Started page.","title":"Where to Start?"},{"location":"#detailed-documentation","text":"To learn more about the various ways gbq can be used, read the Usage Guide page.","title":"Detailed Documentation"},{"location":"changelog/","text":"../CHANGELOG.md","title":"Changelog"},{"location":"development-guide/","text":"Development Guide Welcome! Thank you for wanting to make the project better. This section provides an overview on how repository structure and how to work with the code base. Before you dive into this, it is best to read: The Code of Conduct The Contributing guide Docker The GBQ project uses Docker to ease setting up a consistent development environment. The Docker documentation has details on how to install docker on your computer. Once that is configured, the test suite can be run locally: docker-compose run --rm test If you want to be able to execute code in the container: docker-compose run --rm devbox ( your code here ) In the devbox environment you'll be able to enter a python shell and import gbq or any dependencies. Debugging The docker container has pdb++ install that can be used as a debugger. (However, you are welcome to set up a different debugger if you would like.) This allows you to easily create a breakpoint anywhere in the code. def my_function (): breakpoint () ... When your the code, you will drop into an interactive pdb++ debugger. See the documentation on pdb and pdb++ for more information. Testing You'll be unable to merge code unless the linting and tests pass. You can run these in your container via: docker-compose run --rm test This will run the same tests, linting, and code coverage that are run by the CI pipeline. The only difference is that, when run locally, black and isort are configured to automatically correct issues they detect. Generally we should endeavor to write tests for every feature. Every new feature branch should increase the test coverage rather than decreasing it. We use pytest as our testing framework. Stages To customize / override a specific testing stage, please read the documentation specific to that tool: PyTest MyPy Black Isort Flake8 Bandit setup.py Setuptools is used to packaging the library. setup.py must not import anything from the package When installing from source, the user may not have the packages dependencies installed, and importing the package is likely to raise an ImportError . For this reason, the package version should be obtained without importing . This is explains why setup.py uses a regular expression to grabs the version from __init__.py without actually importing. Requirements requirements.lock - Lists all direct dependencies (packages imported by the library). requirements-test.txt - Lists all direct requirements needed to run the test suite & lints. Publishing the Package TODO: The project currently only has parts of this process implemented. The CI pipeline does not currently publish the wheel and source distribution to PyPI . For now, this must be done manually. Once the package is ready to be released, there are a few things that need to be done: Start with a local clone of the repo on the default branch with a clean working tree. Run the version bump script with the appropriate part name ( major , minor , or patch ). Example: docker-compose run --rm bump minor This wil create a new branch, updates all affected files with the new version, and commit the changes to the branch. Push the new branch to create a new pull request. Get the pull request approved. Merge the pull request to the default branch. Double check the default branch has all the code that should be included in the release. Create a new GitHub release. The tag should be named vX.Y.Z to match the new version number. This will trigger the CI system to build a wheel and a source distributions of the package and push them to PyPI . Continuous Integration Pipeline TODO: Add CI documentation.","title":"Development Guide"},{"location":"development-guide/#development-guide","text":"Welcome! Thank you for wanting to make the project better. This section provides an overview on how repository structure and how to work with the code base. Before you dive into this, it is best to read: The Code of Conduct The Contributing guide","title":"Development Guide"},{"location":"development-guide/#docker","text":"The GBQ project uses Docker to ease setting up a consistent development environment. The Docker documentation has details on how to install docker on your computer. Once that is configured, the test suite can be run locally: docker-compose run --rm test If you want to be able to execute code in the container: docker-compose run --rm devbox ( your code here ) In the devbox environment you'll be able to enter a python shell and import gbq or any dependencies.","title":"Docker"},{"location":"development-guide/#debugging","text":"The docker container has pdb++ install that can be used as a debugger. (However, you are welcome to set up a different debugger if you would like.) This allows you to easily create a breakpoint anywhere in the code. def my_function (): breakpoint () ... When your the code, you will drop into an interactive pdb++ debugger. See the documentation on pdb and pdb++ for more information.","title":"Debugging"},{"location":"development-guide/#testing","text":"You'll be unable to merge code unless the linting and tests pass. You can run these in your container via: docker-compose run --rm test This will run the same tests, linting, and code coverage that are run by the CI pipeline. The only difference is that, when run locally, black and isort are configured to automatically correct issues they detect. Generally we should endeavor to write tests for every feature. Every new feature branch should increase the test coverage rather than decreasing it. We use pytest as our testing framework.","title":"Testing"},{"location":"development-guide/#stages","text":"To customize / override a specific testing stage, please read the documentation specific to that tool: PyTest MyPy Black Isort Flake8 Bandit","title":"Stages"},{"location":"development-guide/#setuppy","text":"Setuptools is used to packaging the library. setup.py must not import anything from the package When installing from source, the user may not have the packages dependencies installed, and importing the package is likely to raise an ImportError . For this reason, the package version should be obtained without importing . This is explains why setup.py uses a regular expression to grabs the version from __init__.py without actually importing.","title":"setup.py"},{"location":"development-guide/#requirements","text":"requirements.lock - Lists all direct dependencies (packages imported by the library). requirements-test.txt - Lists all direct requirements needed to run the test suite & lints.","title":"Requirements"},{"location":"development-guide/#publishing-the-package","text":"TODO: The project currently only has parts of this process implemented. The CI pipeline does not currently publish the wheel and source distribution to PyPI . For now, this must be done manually. Once the package is ready to be released, there are a few things that need to be done: Start with a local clone of the repo on the default branch with a clean working tree. Run the version bump script with the appropriate part name ( major , minor , or patch ). Example: docker-compose run --rm bump minor This wil create a new branch, updates all affected files with the new version, and commit the changes to the branch. Push the new branch to create a new pull request. Get the pull request approved. Merge the pull request to the default branch. Double check the default branch has all the code that should be included in the release. Create a new GitHub release. The tag should be named vX.Y.Z to match the new version number. This will trigger the CI system to build a wheel and a source distributions of the package and push them to PyPI .","title":"Publishing the Package"},{"location":"development-guide/#continuous-integration-pipeline","text":"TODO: Add CI documentation.","title":"Continuous Integration Pipeline"},{"location":"getting-started/","text":"Getting Started Installation To install gbq , simply run this simple command in your terminal of choice: python -m pip install gbq Introduction gbq was developed internally at Wayfair to be used a CD pipeline to create/update tables on Google BigQuery. At Wayfair gbq is used in a with a BigQuery plugin which reads json schema files and updates the schemas on Google BigQuery. What's Next? TODO","title":"Getting Started"},{"location":"getting-started/#getting-started","text":"","title":"Getting Started"},{"location":"getting-started/#installation","text":"To install gbq , simply run this simple command in your terminal of choice: python -m pip install gbq","title":"Installation"},{"location":"getting-started/#introduction","text":"gbq was developed internally at Wayfair to be used a CD pipeline to create/update tables on Google BigQuery. At Wayfair gbq is used in a with a BigQuery plugin which reads json schema files and updates the schemas on Google BigQuery.","title":"Introduction"},{"location":"getting-started/#whats-next","text":"TODO","title":"What's Next?"}]}